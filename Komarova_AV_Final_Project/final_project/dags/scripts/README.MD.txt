I. 
script_1_parsing.py - питоновский код для парсинга, обработки и загрузки первичного слепка данных, 
используемый в DAG final_project.py  

В нем происходит парсинг данных и сохранение "грязной версии", сохранение словаря sites, содержащего тексты 
последних спарсенных новостей для каждого сайта. Это необходимо для последующего инкрементального режима.

Производится обработка данных: уменьшение кол-ва категорий, создание чистеньких датафреймов, 
для последующей загрузки в БД.

Создаются три таблицы: 
1. таблица categories со списком категорий новостей и суррогатными ключами (у нас их 13, 
было принято решение все непопулярные категории помещать в категорию "Всякое другое". 
2. таблица sites со списком сайтов (у нас их 4) и суррогатными ключами
3. таблица ss_new с пронумерованными новостями (первичный ключ), номеро категории (вторичный ключ),
номером сайта (вторичный ключ), датой новости, днем недели, когда была опубликована.

ER-диаграмму, а также сами таблицы можно посмотреть в папке images. Тебе сюда.

II.
parsing_cleaning_and_creating_tables.py - питоновский код для парсинга, 
обработки и загрузки дельты данных, для инкрементального режима используемый в DAG final_project_dag.py

Данный режим не сильно отличается от предыдущего. 
Основные отличия:
1. Загрузка не всего сайта, а только начиная с последней новости (список sites).
2. Загрузка новых данных только в таблицу SS_NEWS, тк сайты и категории у нас не изменяются.


III.
В sgl_sqripts.sql - можно посмотреть скрипты написания итоговых витрин. Получилось 4 витрины. 

Получившиеся витрины можно посмотреть в папке images. Тебе сюда.

IV.
Файлы df_init и data_dirty - то, что сохраняется в первозданном виде. 
По сути это что-то среднее между Mini-Data-Lake и DWH.